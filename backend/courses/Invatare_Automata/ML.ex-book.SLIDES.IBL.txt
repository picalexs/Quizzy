***************Beginning Page***************
***************page number:1**************
1.
Q
Instance-Based Learnmg
Contains:
ex. 1, 4, 21, 11, 9, 10 and 27, 15
from the 2023f version of the ML exercise book by L. Ciortuz et a1.

***************Ending Page***************

***************Beginning Page***************
***************page number:2**************
2.
The k-NN algorithm: simple application
CMU, 2006 fall, ﬁnal exam, pr. 2
y
Consider the training set
in the 2-dirnensional Eu- —- 3 '
clidean space shown in —1 1 — /________\_
the nearby table. 0 1 + f/ \"x
. . 0 2 — 2, /.\\ Q
a. Represent the tralnlng 1 _1 _ // / \\ \\
data in the 2D space. 1 0 + 1"‘ 1/ \\ ‘\z
b. What are the pre- 1 2 _|_ (3:) ‘I 1 >< \| ‘i
dictions of the 3- 5- and 2 2 _ l ‘\ ,' f x:
7-nearest-neighbor classi- 2 3 + \=\ \\ // f’
ﬁers at the point (1,1)? i ‘\ \ ’/ / "
Solution: ~_
—1_*~~-~----"”o
b. 142:3: +; kIE'): +; k:7: —.

***************Ending Page***************


***************Beginning Page***************
***************page number:3**************
3.
Drawing decision boundaries and decision surfaces
for the 1-NN classiﬁer
Voronoi Diagrams
CMU, 2010 spring, E. Xing, T. Mitchell, A. Singh,
HWl, pr. 3.1

***************Ending Page***************

***************Beginning Page***************
***************page number:4**************
4.
‘ 4,4 ‘ 4,4

For each 0f these °
ﬁgures, we are
given a few data ' '
points in 2-d space,
each 0f which is
labeled as either o o
positive (blue) 0r
negative (red). _4’_4 5 _4,_4 5
Assuming that we 4,4 4,4
are using the L2 o
distance as a dis- O O °
tance metric, draw O _ O _
the decision bound- O
ary for the 1-NN
classiﬁer for each O O
case. '_

_4,_4 1 _4,_4 1

***************Ending Page***************


***************Beginning Page***************
***************page number:5**************
S l t'
44 44

1 Q 1 CR 5
1 \ I \ ~
1 \1 X:
1 \1 '

1 1\ :\\

1 1\\ o .\

1 /, 1 ,b Z 9’

I / I / I//
1/ ‘/ ~

1/ 3/ 1/
1/ /I /.
/ I / I / I
/ ‘ / ‘ / ~
/ 1 / I / I
/ / / .
/ 1 , 1 /
/ 1 , 1 , 1
/ ; / 1 / :
o’ 3 d 1 q 5
1 ; \ :
1 1 \ :
I 1 \ I
1 f o :
1 _4_4 :
—4,—4 ’ —4,—4

***************Ending Page***************

***************Beginning Page***************
***************page number:6**************
4,4
O 0? I
\ '___ ——___
\ . z/
\ //
IV’
9—___ / . \
___/_/__ | \
/:__- —-:-::'3'
GQ' . /
\\ ' /
\\ |
\ ,/
/\
_______________________ _,/___:_ __\_\________________________
\
| \\
| \
\
/ ' \\
/ \
/ I \.
/ }
/ I
/ I
/ \ _-_—___
/ \ j Q
/ \ '
/ .
/ \ ' .
/ \ I
d \ '
O I
-4,-4

***************Ending Page***************


***************Beginning Page***************
***************page number:7**************
7.
Drawing decision boundaries and decision surfaces
for the 1-NN classiﬁer
Voronoi Diagrams: DO IT YOURSELF
CMU, 2010 fall, Ziv Bar-Joseph, HWl, pr. 3.1

***************Ending Page***************

***************Beginning Page***************
***************page number:8**************
8.
2 2
For each of the 1'5 1'5
nearby ﬁgures, you 1 O + 1 O O
are given negative 0.5 0.5
(o) and positive (+) 0 0 _|_
data points in the
-0.5 -0.5
2D space.
‘1 O + ‘1 O O
Remember that a 1- _1 5 _1 5
NN classiﬁer classi- I I
ﬁes a point accord- _2-2 -1.5 _1 -0.5 0 0.5 1 1.5 2 _2-2 -1.5 -1 -o.5 0 0.5 1 1.5 2
ing to the class of its
nearest neighbour. 2 2
1.5 1.5
Please draw the
Voronoi diagram 1 O O 1 O O + +
for a 1-NN classiﬁer 0-5 0-5 + O
. . O +
usmg Euclldean 0 + + 0
~ O
dlstance as the _0_5 + -0.5 +
distance metric for 1 1
each case. Q
_1.5 -1.5 O
_2 -2
-2 -1.5 -1 -0.5 0 0.5 1 1.5 2 -2 -1.5 -1 -o.5 o 0.5 1 1.5 2

***************Ending Page***************


***************Beginning Page***************
***************page number:9**************
9.

2

1.5

1 O + +

O
0.5 + C)
_|_

0
-0.5 +

-1

O

-1.5 O

-2

-2 -1.5 -1 -0.5 0 0.5 1 1.5 2

***************Ending Page***************

***************Beginning Page***************
***************page number:10**************
10.
Decision boundaries and decision surfaces:
Comparison between the 1-NN and ID3 classiﬁers
CMU, 2007 fall, Carlos Guestrin, HW2, pr. 1.4

***************Ending Page***************


***************Beginning Page***************
***************page number:11**************
11.

For the data in the ﬁgure(s) below, sketch the decision surfaces obtained by
applying
a. the K-Nearest Neighbors algorithm with K : 1;
b. the ID3 algorithm augmented with [the capacity to process] continuous
attributes.

y y

6 6

5 o o 5 o o

4 o 4 o

3 o o o 3 o o o

2 o o 2 o o

1 o o 1 o o

O O

O 1 2 3 4 5 6 x O 1 2 3 4 5 6 x

***************Ending Page***************

***************Beginning Page***************
***************page number:12**************
12.
Solution: l-NN
y y
6 6
2 "'f'“ 1:119 o
1..‘Ii"’"" 1
O 0
0 1 2 3 4 5 6 x 0 1 5 6 x

***************Ending Page***************


***************Beginning Page***************
***************page number:13**************
13.
Solution: ID3
Z y
6
5 o O 5 O
. O O
I o 1
() 0
0 1 2 3 4 5 6 x 0 1 4 5 6 x
E

***************Ending Page***************

***************Beginning Page***************
***************page number:14**************
Instance-Based Learning
Some important properties

***************Ending Page***************


***************Beginning Page***************
***************page number:15**************
15.
k-NN and the Curse 0f Dimensionality
Proving that the number of examples needed by k-NN
grows exponentially with the number of features
CMU, 2010 fall, Aarti Singh, HW2, pr. 2.2
[Slides originally drawn by Diana Minzat, MSc student, FII, 2015 spring]

***************Ending Page***************

***************Beginning Page***************
***************page number:16**************
16.
Consider a set of n points x1, 11:2, ..., atn independently and uniformly
drawn from a p-dimensional zero-centered unit ball
B I {96: ||wr|2 s 1} c R12
where HxH I Vx- a; and - is the inner product in RP.
In this problem we will study the size of the 1-nearest neigh-
bourhood of the origin O and how it changes in relation to the
dimension p, thereby gain intuition about the downside of k-NN
in a high dimension space.
Formally, this size will be described as the distance from O to its
nearest neighbour in the set {231, ..., inn}, denoted by d*:
d*:: min H5131||,
1§i§n
which is a random variable since the sample is random.

***************Ending Page***************


***************Beginning Page***************
***************page number:17**************
17.
a. For p I 1, calculate P(d* g t), the cumulative distribution
function (c.d.f.) of d*, for t € [0,1].
Solution:
In the one-dimensional space (p I 1), the unit ball is the interval
[—1,1]. The cumulative distribution function will have the follow-
ing expression:

Fn,1(t) nIt' P(d* g t) I 1 — P(d* >15) I 1— P(\a:Z-| > t, for i I 1, 2, ...,n)
Because the points 51:1,...,xn were generated independently, the
c.d.f. can also be written as:

Fn,1(1:):1- 11PM >t):1_(1 - t)”
1:1

***************Ending Page***************

***************Beginning Page***************
***************page number:18**************
18.
b. Find the formula 0f the cumulative distribution function of d*
for the general case, when p E {1, 2, 3, ....}
Hint: You may ﬁnd the following fact useful: the volume of a
p-dimensional ball with radius r is
WW’
m) I p ,
r (— +1)
2

where P is Euler’s Gamma function, deﬁned by

1

F (5) I ﬁ, U1) I l, and Ha: + l) I gtP($) for any 55 > O.

Note: It can be easily shown that F(n + 1) I n! for all n € N*,
therefore the Gamma function is a generalization of the factorial
function.

***************Ending Page***************


***************Beginning Page***************
***************page number:19**************
19.
Solution:
In the general case, i.e. considering a ﬁxed p € N*, it is obvious that the
cumulative distribution function of d* will have a similar form to the p I 1
case:
FWQ) "25- P(d* g 1:) I 1- P(d* >t):1- P(||wZ-H >1; i:1,2,...,n)
I 1—HP(\|xZ-H > t).
1:1
Denoting the volume of the sphere of radius t by 1/},(25), and knowing that
the points x1, ..., sun follow a uniform distribution, we can rewrite the above
formula as follows:
V 1 — V 1: n V t n
Fn7p(t):1_< p<> po) :1_ (1_ p<>)_
‘69(1) ‘69(1)
Using the suggested formula for the volume of the sphere, it follows im-
mediately that Fm) I 1 — (1 — 15p)”.

***************Ending Page***************

***************Beginning Page***************
***************page number:20**************
20.
c. What is the median of the random variable d* (i.e., the value
of t for which P(d* g t) : 1/2) ‘.7 The answer should be a function
of both the sample size n and the dimension p.
Fix n I 100 and plot the values of the median function for p :
1, 2,3, ..., 100 with the median values on the y-aXis and the values
of p on the x-axis. What do you see?
Solution:
In order to ﬁnd the median value of the random variable d*, we
will solve the equation P(d* g t) I 1/2 of variable t:
1 1 1 1
P(d*gt):- e FWQ):-é>1_(1—tp)":-e(1—tp)":-
2 ’ 2 2 2
1 1
_ p I _ p I _ _
e 1 t 21/7111” 1 21m
Therefore,
1 1/19
tmed(nap) I (1— 21w) -

***************Ending Page***************


***************Beginning Page***************
***************page number:21**************
. 21.
The plot of the functlon tm6d(100,p) for p : 1, 2, . . . , 100:
1
Remark:
0.8
The minimal sphere contain-
ing the nearest neighbour
if 0-6 of the origin in the set
5 {$1,x2, ...,xn} grows very fast
v? as the value of p increases.
n 0.4
When p becomes greater
than 10, most of the 100
02 training instances are closer
to the surface of the unit ball
than to the origin O.
00 20 40 60 80 100
p

***************Ending Page***************

***************Beginning Page***************
***************page number:22**************
22.
d. Use the c.d.f. derived at point b to determine how large should
the sample size n be such that with probability at least 0.9, the
distance d* from O to its nearest neighbour is less than 1/2, i.e.,
half way from O to the boundary of the ball.
The answer should be a function of p.
Plot this function for p I 1,2, . . .,2() with the function values on
the y-axis and values of p on the at-axis. What do you see?
Hint: You may ﬁnd useful the Taylor series expansion of ln(1 — :13):
OO $7;
l 1 — : — — f — 1 < < 1.
n( x) g 7; or _ gr

***************Ending Page***************


***************Beginning Page***************
***************page number:23**************
23.
Solution:
9 b 1 n 9 1 n 1
Pd*<. >. Fn . >_'1-1-— >_ 1—— <_
( _05)_09 (I) ,p(05)_1O<:> ( 2p) _10<:>( 2p) _10
1 1n10
¢> n-1n1—— g —1n10¢>n2 —
2p 1
—1n(1——)
2p
Using the decomposition of 1n(1 — 1/2p) into a Taylor series (with x I 1/2p),
weobtain:
P(d* g 0.5) z 0.9
1
p —
in2(1n10)21 1 1 1 1 1 1
:> n2 2P—11n10.

***************Ending Page***************

***************Beginning Page***************
***************page number:24**************
24.
Note:
In order to obtain the last inequality in the above calculations, we
considered the following two facts:
1 1
i. m < Z holds for any p Z 1, and
1 1
ii. — g — (I) 2” g n - 2(”_1)p holds for any p Z l and n Z 2.
(This can be proven by induction on p).
So, we got:
1 1 1 1 1 1 1
+2.2—P+§'%+'H+EW+H'<
1 + 1 + 1 + + 1 + % 1 — 2
2 4 2n 1_l_‘
2

***************Ending Page***************


***************Beginning Page***************
***************page number:25**************
25.
2.5
2 l
The proven result
(DO
P(d* g 0.5) z 0.9 :> n 2 219-1 lnlO £15
'cxl
means that the sample size needed 1;,
for the probability that d* < 0.5 is g 1
large enough (9/10) grows expo- é
nentially with p. '
0.5
0 A i
O 5 1O 15 20
p

***************Ending Page***************

***************Beginning Page***************
***************page number:26**************
26.
e. Having solved the previous problems, what will you say about
the downside of k-NN in terms of n and p?
Solution:
The k-NN classiﬁer works well when a test instance has a “dense”
neighbourhood in the training data.
However, the analysis here suggests that in order to provide a
dense neighbourhood, the size of the training sample should be
exponential in the dimension p, which is clearly infeasible for a
large p.
(Remember that p is the dimension of the space we work in, i.e.
the number of features of the training instances.)

***************Ending Page***************


***************Beginning Page***************
***************page number:27**************
27.
An upper bound for the assimptotic error rate of 1-NN:
twice the error rate of Joint Bayes
T. Cover and P. Hart (1967)
CMU, 2005 spring, Carlos Guestrin, HW3, pr. 1

***************Ending Page***************

***************Beginning Page***************
***************page number:28**************
28.
Note: we Will prove the Covert 63 Hart’ theorem in the case of
binary classiﬁcation with real-values inputs.
Let x1, x2, . . . be the training examples in some ﬁxed d-dimensional
Euclidean space, and y,- be the corresponding binary class labels,
Let py(x) mg. P(X = x | Y = y) be the true conditional probability
distribution for points in class y. We assume continuous and non-
zero conditional probabilities: 0 < py(:1:) < 1 for all a: and y.
Let also <9 mg. P(Y I 1) be the probability that a random training
example is in class 1. Again, assume O < 9 < 1.

***************Ending Page***************


***************Beginning Page***************
***************page number:29**************
29.
a. Calculate q(x) 712.]?(3/ = 1 | X I x), the true probability that a
data point a: belongs to class 1. Express (1(513) in terms of p0(:1:),p1(x),
and H.
Solution:
P(X — :13)

_ P(X I x|Y :1)P(Y :1)

_ P(X I ny I 1)P(Y :1)+ P(X I my I 0)P(Y I 0)

I p1(x) 6’

p1(x) 6 +p0(x)(1 — 8)

***************Ending Page***************

***************Beginning Page***************
***************page number:30**************
30.
b. The Joint Bayes classiﬁer (usually called the Bayes Optimal
classiﬁer) always assigns a data point x the most probable class:
argmaxy P(Y I y | X I x).
Given some test data point x, what is the probability that example
a: will be misclassiﬁed using the Joint Bayes classiﬁer, in terms of
(1(56)?
Solution:
The Joint Bayes classiﬁer fails with probability P(Y I OIX I x)
when P(Y I 1|X I x) Z P(Y I OIX I :13), and respectively with
probability P(Y I 11X I x) when P(Y I OlX I x) Z P(Y I llX I x).
I.e.,
ErrorBayes(x) I min{P(Y I 0|X I at), P(Y I 1|X I 515)}
= min{1 — (1(93), q(w)}
I q<x> if q<x> e [0, 1/21
1 - q<w> if q<w> e <1/2, 11-

***************Ending Page***************


***************Beginning Page***************
***************page number:31**************
31.

c. The 1-nearest neighbor classiﬁer assigns a test data point x

the label of the closest training point r’.

Given some test data point r and its nearest neighbor r’, What

is the empected error of the 1-nearest neighbor classiﬁer, i.e., the
probability that a: will be misclassiﬁed, in terms of q(:v) and q(a7’)?
Solution:

Err0r1_NN(at) I P(Y I llX I $)P(Y I OIX I 95') +
P(Y I 0|X I x)P(Y I llX I 51:’)
I Q($)<1_ (1WD + (1 — Q($))Q($l)-

***************Ending Page***************

***************Beginning Page***************
***************page number:32**************
32.

d. In the asymptotic case, i.e. when the number of training ex-
amples of each class goes to inﬁnity, and the training data ﬁlls
the space in a dense fashion, the nearest neighbor :13’ of a: has q(a:’)
converging to q(r), i.e. P(Y I l|X I 55’) e p(Y I llX I x).
(This is true due to i. the result obtained at the above point a, and
ii. the assumed continuity of the function py(:1:) nIt' p(X I atlY I y)
W.r.t. :13.)
By performing this substitution in the expression obtained at
point c, give the asymptotic error for the 1-nearest neighbor clas-
siﬁer at point r, in terms of (1(90).
Solution:

lim Err0r1_NN($) I 2q(:13)(l — q(x))

$’—>:I:

***************Ending Page***************


***************Beginning Page***************
***************page number:33**************
33.
e. Show that the asymptotic error obtained at point d is less than
twice the Joint Bayes error obtained at point b and subsequently
that this inequality leads to the corresponding relationship be-
tween the expected error rates:
E[ lirn Err0r1_NN] g 2E[Err0r5ay65].
n—>OO
Solution:
z(1 — z) g z for all z, in particular for z E [0, 1/2], and 1
2(1 — z) g 1 — z for all z, in particular for z € [1/2, 1].
Therefore, for all x,
q<w> if q<w> e [0, 1/21
q(fv)(1 — (1W) ﬁ .
1 — q(x) 1f (Km) G (1/2, 1]. 0.5
The results obtained at points b and d lead to A
lirn Err0r1_NN(:13) I 2q(x)(1 — q(:13)) g 2Err0r5ayes(:r) for all x. ‘
TL—>OO
O
By multiplication with P(:13) and suming upon all values of 0 025 1
ZU, we get: E[li1'nn_>OO Err0r1_NN] § 2E[Err0r5ayes].

***************Ending Page***************

***************Beginning Page***************
***************page number:34**************
34.
Remarks
0 E [limn_>OO Err0r1_NN] Z E [ErrorBayw]
Proof:
2z—2z2 Z z Vz €[(),1/2] and 2z—2,z2 Z 1—z V2 € [1/27 1].
Therefore,
2q(x)(l — q(x)) Z ErrorBayes(x) for all x,
and
lim Err0r1_NN(x) I lim Err0r1_NN(x) Z ErrorBayes(x) for all x.
n—>oo x’—>a:
0 The Cover & Hart’ upper bound for the asymptotic error rate
of 1-NN doesn’t hold in the non-asymptotic case (Where the
number of training examples is ﬁnite).

***************Ending Page***************


***************Beginning Page***************
***************page number:35**************
35.
Remark
from: An Elementary Introduction to Statistical Learning Theory,
S. Kulkarni, G. Harman, 2011, pp. 68-69
An even tighter upper bound exists for E[limn_>oo Err0r1_NN]:
2E[E"rror5ay€8] (1 — E [Errorgayesh

Proof:
From limm/hm Error1_NN(a3) I 2q(:c)(l — q(a:)) (see point d) and
ErrorBayes(w) : min{1 — q(x), q(:13)} (see point b),
it follows that
hinge/hm Error1_NN(:t) I ZETTOTBay€S(ZU)(1 — ErrorBayes(x)).
By multiplying this last equality with P(a:) and suming on all x — in fact,
integrating upon w —, we get
E[ lim Error1_NN] : 2E[ErrorBayes(1 — ErrorBayes)] I 2E[ErrorBayes] — 2E[(Error3ayes)2].

:6’—>a;
Since E[ZQ] 2 (E[Z])2 for any Z (mu) dif- E[(Z—E[Z])2] 002” E[Z2]—(E[Z])2 z 0),
it follows that
E[ lim Error1_NN] g 2E[ErrorBay65] — 2(E[Err0"/‘Bay65])2 I 2E[Error3ayes](l — E[ErrorBayeS]).

x’—>a:

***************Ending Page***************

***************Beginning Page***************
***************page number:36**************
36.
Other Results
[from An Elementary Introduction to Statistical Learning Theory,
S. Kulkarni, G. Harman, 2011, pp. 69-70]
0 When certain restrictions hold,
1
E[ lirn Errork_NN] g (1 —|— E) E[Error3ayes].
TL—>OO
o However, it can be shown that there are some distributions for which
l-NN outperforms k-NN for any ﬁxed k > 1.
kn .
0 If — e 0 for n —> 00 (for 1nstance, kn I ﬁ), then
n
E [ lirn Errorkn_NN] I E [ErrorBayes].
TL—>OO

***************Ending Page***************


***************Beginning Page***************
***************page number:37**************
37.
Signiﬁcance
The last result means that kn-NN is
— a universally consistent learner (because when the amount of
training data grows, its performance approaches that of Joint
Bayes) and
— non-parametric (i.e., the underlying distribution of data can
be arbitrary and we need no knowledge of its form).
Some other universally consistent learners exist.
However, the convergence rate is critical. For most learning meth-
ods, the convergence rate is very slow in high-dimensional spaces
(due to “the curse of dimensionality”). It can be shown that there
is no “universal” convergence rate, i.e. one can always ﬁnd dis-
tributions for which the convergence rate is arbitrarily slow.
There is no one learning method which can universally beat out
all other learning methods.

***************Ending Page***************

***************Beginning Page***************
***************page number:38**************
38.
Conclusion
Such results make the ML ﬁeld continue to be exciting,
and makes the design of good learning algorithms and
the understanding of their performance an important
science and art!

***************Ending Page***************


***************Beginning Page***************
***************page number:39**************
39.
On 1-NN and kernelization with RBF
CMU, 2003 fall, T. Mitchell, A. Moore, ﬁnal exam, pr. 7.f

***************Ending Page***************

***************Beginning Page***************
***************page number:40**************
40.
After mapped into the feature space Q through a radial basis ker-
nel function (RBF), the 1-NN algorithm using unweighted Eu-
clidean distance may be able to achieve a better classiﬁcation per-
formance than in the original space (though we can’t guarantee
this). True or False?

***************Ending Page***************


***************Beginning Page***************
***************page number:41**************
Answer 41'
n0. _M .
Consider <15 : Rd —> R” such that K(x,y) :t e 2J3 I (Mm) -q5(y),Vat,y 6 Rd. Rd 1s
||w—y|\2
the original space, R” is the “feature” space, and 6- 202 is the radial basis
function (RBF). Then
ll¢(f1§) — WHIP I (My?) — ¢(y)) ' (M93) — $04))
_||Q@—@@||2 _||y—y||2 _||@c—y||2
I (pm) - $01") +¢(y) -¢(y) — 2 - ¢($) -¢(y) I 6 202 +e 2” — 2 - 6 262
\|w—y||2 ||w—y||2
I eO+eO—2-e_ 202 :2—2-e_ 262 :2—K(a:,y)
Suppose a3,- and x,- are two neighbors for the test instance a: such that ||:13—$,-|| <
||a3 — xjH. After mapped to the feature space,
Wm) — ¢(iv@)||2 < WW‘) — ¢($1)||2 <I> 2 — KGB, Ha) < 2 — KWﬁj) <I> KWQJU > KW $1)
||Qc_g@,-||2 ||17_93j||2 _ i 2 _ - 2
e a > a e _% > -% e \|x—x,'||2 < ||$ -$,-||2.
So, if as,- is the nearest neighbor of a: in the original space, it will also be the
nearest neighbor in the feature space. Therefore, 1-NN doesn’t work better
in the feature space. (The same is true for k-NN.)
Note: k-NN using non-Euclidean distance or weighted voting may work.

***************Ending Page***************





