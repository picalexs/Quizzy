[{content={parts=[{text=--FlashCardSeparator--
Single
--InteriorSeparator--
What is the primary characteristic of instance-based learning?
--InteriorSeparator--
Storing all training examples and computing the target function locally during classification.
--InteriorSeparator--
easy
--InteriorSeparator--
2
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
Which of the following are advantages of instance-based learning?
--InteriorSeparator--
(right) Can learn very complex target functions
(right) Training is very fast
(wrong) Always provides the optimal solution
(wrong) Insensitive to noisy data
--InteriorSeparator--
medium
--InteriorSeparator--
2
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is a major disadvantage of instance-based learning methods during query time?
--InteriorSeparator--
Slow speed.
--InteriorSeparator--
easy
--InteriorSeparator--
2
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
Which of the following methods are considered instance-based learning approaches?
--InteriorSeparator--
(right) k-Nearest Neighbor
(wrong) Decision Trees
(right) Locally weighted regression
(wrong) Genetic Algorithms
--InteriorSeparator--
medium
--InteriorSeparator--
3
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
In k-NN classification with discrete-valued f, how is the prediction made?
--InteriorSeparator--
By taking a vote among the k nearest neighbors.
--InteriorSeparator--
easy
--InteriorSeparator--
4
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is a Voronoi Diagram useful for illustrating in the context of k-NN?
--InteriorSeparator--
The decision surface induced by 1-NN.
--InteriorSeparator--
medium
--InteriorSeparator--
5
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
When should k-Nearest Neighbor be considered as a suitable learning method?
--InteriorSeparator--
(right) Instances map to points in n-dimensional space
(wrong) When dealing with a small dataset
(right) Less than 20 attributes per instance
(wrong) When high interpretability is required
--InteriorSeparator--
medium
--InteriorSeparator--
6
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is the purpose of using kd-trees in k-NN?
--InteriorSeparator--
To efficiently retrieve the nearest neighbors.
--InteriorSeparator--
medium
--InteriorSeparator--
7
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is the "Curse of Dimensionality" in the context of k-NN?
--InteriorSeparator--
Irrelevant attributes dominating the decision due to high dimensionality.
--InteriorSeparator--
hard
--InteriorSeparator--
8
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
How can the Curse of Dimensionality be mitigated in k-NN?
--InteriorSeparator--
(wrong) Reducing the number of training examples
(right) Stretching the axes by weights to minimize prediction error
(wrong) Ignoring all dimensions with high variance
(right) Setting weights of irrelevant dimensions to zero
--InteriorSeparator--
hard
--InteriorSeparator--
8
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
In distance-weighted k-NN, how are nearer neighbors weighted compared to farther neighbors?
--InteriorSeparator--
Nearer neighbors are weighted more heavily.
--InteriorSeparator--
medium
--InteriorSeparator--
9
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What happens to distance-weighted k-NN when all training examples are used instead of just *k*?
--InteriorSeparator--
It becomes Shepard's method.
--InteriorSeparator--
medium
--InteriorSeparator--
9
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
What does k-NN approach in the limit (as the number of training examples approaches infinity and k gets large)?
--InteriorSeparator--
(right) The Bayes optimal learner
(wrong) A decision tree
(wrong) Linear Regression
(wrong) The Gibbs algorithm
--InteriorSeparator--
hard
--InteriorSeparator--
10
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What does 1-NN approach in the limit (as the number of training examples approaches infinity)?
--InteriorSeparator--
The Gibbs algorithm.
--InteriorSeparator--
hard
--InteriorSeparator--
10
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is the key idea behind Locally Weighted Regression?
--InteriorSeparator--
Forming an explicit approximation to the target function f for the region surrounding the query point.
--InteriorSeparator--
medium
--InteriorSeparator--
11
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
What type of function is typically fitted to the k nearest neighbors in Locally Weighted Regression?
--InteriorSeparator--
(right) Linear function
(wrong) Step function
(right) Quadratic function
(wrong) Exponential function
--InteriorSeparator--
medium
--InteriorSeparator--
11
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is the role of the kernel function in distance-weighted squared error calculation for locally weighted regression?
--InteriorSeparator--
It decreases over distance to give closer points more weight.
--InteriorSeparator--
hard
--InteriorSeparator--
12
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
Describe the key difference between Radial Basis Function Networks and distance-weighted regression.
--InteriorSeparator--
RBF Networks are "eager" learners while distance-weighted regression is "lazy."
--InteriorSeparator--
hard
--InteriorSeparator--
13
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
What is the output of Radial Basis Function Networks computed as?
--InteriorSeparator--
(wrong) A majority vote of the kernel functions
(right) A linear combination of kernel functions
(wrong) The inverse of the kernel functions
(wrong) An exponential decay of the kernel functions
--InteriorSeparator--
medium
--InteriorSeparator--
13
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What assumption makes Radial Basis Function Networks well-suited for image classification?
--InteriorSeparator--
The assumption of spatially local influences.
--InteriorSeparator--
medium
--InteriorSeparator--
13
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is commonly chosen as the kernel function in Radial Basis Function Networks?
--InteriorSeparator--
Gaussians.
--InteriorSeparator--
medium
--InteriorSeparator--
14
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
What are the two main questions to consider when training Radial Basis Function Networks?
--InteriorSeparator--
(right) What to use for each kernel function
(wrong) Which activation function to use
(right) How to train the weights
(wrong) How many layers to use in the output layer
--InteriorSeparator--
medium
--InteriorSeparator--
15
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
According to the Hartman et al., 1990 theorem, under what conditions can a function be approximated with arbitrarily small error using RBF Networks?
--InteriorSeparator--
Provided a sufficiently large k and the width of each kernel can be separately specified.
--InteriorSeparator--
hard
--InteriorSeparator--
16
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is Case-Based Reasoning (CBR)?
--InteriorSeparator--
Instance-based learning applied to instance spaces with rich symbolic logic descriptions.
--InteriorSeparator--
hard
--InteriorSeparator--
17
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
In which domains has Case-Based Reasoning (CBR) been applied?
--InteriorSeparator--
(right) Conceptual design of mechanical devices
(wrong) Stock market prediction
(right) Reasoning about new legal cases
(wrong) Natural language processing
--InteriorSeparator--
hard
--InteriorSeparator--
17
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is the connection between instance-based learning and the inductive bias?
--InteriorSeparator--
The classification of a query instance will be most similar to the classification of nearby training instances.
--InteriorSeparator--
medium
--InteriorSeparator--
2
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
Which of the following are considered solutions for k-NN being misled by irrelevant attributes?
--InteriorSeparator--
(wrong) Increasing the value of k
(right) Stretching the j-th axis by weight zj
(right) Use cross-validation to choose values for the weights
(wrong) Using only the closest neighbor
--InteriorSeparator--
hard
--InteriorSeparator--
8
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
How are the at are the attributes describing the instances in Radial Basis Function Networks used?
--InteriorSeparator--
These are used in computing the distance measure d(atu, x).
--InteriorSeparator--
medium
--InteriorSeparator--
14
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
In training Radial Basis Function Networks, how can the means (and possibly variances) of the kernel functions Ku be automatically chosen?
--InteriorSeparator--
Using the EM algorithm.
--InteriorSeparator--
hard
--InteriorSeparator--
15
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
What is the training rule wj for minimizing the error in Locally Weighted Regression using gradient descent with k nearest neighbors, where:
wj is the weight for attribute j
K(d(xq, x)) is the kernel function of the distance between the query point xq and a neighboring point x
f(x) is the actual target value for x
f^(x) is the predicted target value for x
aj(x) is the value of attribute j for x
η is the learning rate
--InteriorSeparator--
(wrong) wj ← wj - η Σ K(d(xq, x))(f(x) - f^(x))aj(x)
(right) wj ← wj + η Σ K(d(xq, x))(f(x) - f^(x))aj(x)
(wrong) wj ← wj + Σ K(d(xq, x))(f(x) - f^(x))aj(x)
(wrong) wj ← wj - η Σ K(d(xq, x))(f(x) - f^(x))
--InteriorSeparator--
hard
--InteriorSeparator--
12
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
What is the distance metric 'd' generally used for in both k-NN and Distance-Weighted k-NN?
--InteriorSeparator--
It represents the distance between the query point xq and a training instance xi.
--InteriorSeparator--
medium
--InteriorSeparator--
4, 9
--FlashCardSeparator--

--FlashCardSeparator--
Multiple
--InteriorSeparator--
What are the key steps involved in classifying a new query instance using the k-Nearest Neighbors (k-NN) algorithm?
--InteriorSeparator--
(right) Locate the k nearest training examples
(wrong) Build a decision tree from the training data
(right) Estimate the target function value based on the k nearest neighbors
(wrong) Calculate the information gain for each attribute
--InteriorSeparator--
medium
--InteriorSeparator--
4
--FlashCardSeparator--

--FlashCardSeparator--
Single
--InteriorSeparator--
In k-NN, when classifying a query point, what does argmax represent in the formula `f(xq) <- argmaxvEV Σi=1k I{v = f(xi)}`?
--InteriorSeparator--
It represents the value v in the set of possible values V that maximizes the sum of indicator functions I{v = f(xi)} over the k nearest neighbors. In simpler terms, it finds the most frequent class among the k nearest neighbors.
--InteriorSeparator--
hard
--InteriorSeparator--
4
--FlashCardSeparator--
}], role=model}, finishReason=STOP, avgLogprobs=-0.17833889601269007}]